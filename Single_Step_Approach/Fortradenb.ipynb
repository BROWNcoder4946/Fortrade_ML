{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10228786,"sourceType":"datasetVersion","datasetId":6324235},{"sourceId":10230553,"sourceType":"datasetVersion","datasetId":6325482},{"sourceId":10230696,"sourceType":"datasetVersion","datasetId":6325573},{"sourceId":10230895,"sourceType":"datasetVersion","datasetId":6325711}],"dockerImageVersionId":30806,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"scrolled":true,"execution":{"iopub.status.busy":"2024-12-18T22:13:30.334188Z","iopub.execute_input":"2024-12-18T22:13:30.335136Z","iopub.status.idle":"2024-12-18T22:13:38.046463Z","shell.execute_reply.started":"2024-12-18T22:13:30.335076Z","shell.execute_reply":"2024-12-18T22:13:38.045458Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip uninstall tensorflow tensorboard tensorflow-text tf-keras ml-dtypes -y\n!pip install tensorflow==2.15.0 tensorflow-addons==0.21.0","metadata":{"trusted":true,"scrolled":true,"execution":{"iopub.status.busy":"2024-12-18T22:13:38.048729Z","iopub.execute_input":"2024-12-18T22:13:38.049611Z","iopub.status.idle":"2024-12-18T22:14:55.425782Z","shell.execute_reply.started":"2024-12-18T22:13:38.049565Z","shell.execute_reply":"2024-12-18T22:14:55.424500Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nimport tensorflow_addons as tfa\n\nprint(\"TensorFlow Version:\", tf.__version__)\nprint(\"TensorFlow Addons Version:\", tfa.__version__)","metadata":{"trusted":true,"scrolled":true,"execution":{"iopub.status.busy":"2024-12-18T22:14:55.427617Z","iopub.execute_input":"2024-12-18T22:14:55.427983Z","iopub.status.idle":"2024-12-18T22:15:01.486290Z","shell.execute_reply.started":"2024-12-18T22:14:55.427951Z","shell.execute_reply":"2024-12-18T22:15:01.485171Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install mplfinance\n!pip install Pillow\n!pip install seaborn","metadata":{"trusted":true,"scrolled":true,"execution":{"iopub.status.busy":"2024-12-18T22:15:01.487771Z","iopub.execute_input":"2024-12-18T22:15:01.488573Z","iopub.status.idle":"2024-12-18T22:15:31.282276Z","shell.execute_reply.started":"2024-12-18T22:15:01.488524Z","shell.execute_reply":"2024-12-18T22:15:31.280967Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import mplfinance as fplt\nimport matplotlib\nmatplotlib.use('agg')\nfrom matplotlib import image\nfrom matplotlib import pyplot\nimport pandas as pd\nimport tensorflow as tf\nimport random\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D,BatchNormalization\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.models import load_model\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm \nimport seaborn as sn\nfrom sklearn.metrics import classification_report,accuracy_score\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport tensorflow_addons as tfa","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T22:15:31.286637Z","iopub.execute_input":"2024-12-18T22:15:31.287289Z","iopub.status.idle":"2024-12-18T22:15:32.877133Z","shell.execute_reply.started":"2024-12-18T22:15:31.287251Z","shell.execute_reply":"2024-12-18T22:15:32.876036Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"forex_df_data = pd.read_csv('/kaggle/input/fortrademl/AI_forex_fisichella_garolla_master/Datasets/MarketMovement/EURUSD/Seqs_2000_1_3_0.txt', sep=\";\")\nforex_df_labels = pd.read_csv('/kaggle/input/fortrademl/AI_forex_fisichella_garolla_master/Datasets/MarketMovement/EURUSD/Labels_2000_1_3_0.txt', sep=\";\")\nforex_df = pd.merge(forex_df_data, forex_df_labels, on='IDTime')\nforex_df.rename(columns={'ReadableTime': 'Date'}, inplace=True)\nforex_df['Date']= pd.to_datetime(forex_df['Date']) \nforex_df['Open'] = forex_df['Close']\nforex_df['Open'] = forex_df['Open'].shift(1)\nforex_df.head(5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T22:15:32.878525Z","iopub.execute_input":"2024-12-18T22:15:32.879309Z","iopub.status.idle":"2024-12-18T22:15:33.200649Z","shell.execute_reply.started":"2024-12-18T22:15:32.879262Z","shell.execute_reply":"2024-12-18T22:15:33.199555Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"forex_ohlc = forex_df[[\"Date\",\"Open\",\"High\",\"Low\",\"Close\",\"Class\"]].copy()\ndt_range = pd.date_range(start=\"2010.01.01\", end=\"2020.01.01\", freq='240min')\nforex_ohlc = forex_ohlc[forex_ohlc.Date.isin(dt_range)]\nforex_ohlc.reset_index(drop=True,inplace=True)\nforex_ohlc.head(5)\ntotal_size = forex_ohlc.shape[0]\ntrain_size = int(0.8*total_size)\ntest_size = total_size - train_size\nmin_val = forex_ohlc.Low.min()\nmax_val = forex_ohlc.High.max()\nprint(\"Train set size = {0} and Test set size = {1}\".format(train_size,test_size))\ntrain_df = forex_ohlc.iloc[:train_size,:]\ntest_df = forex_ohlc.iloc[train_size:,:]\n\ntrain_df\n\nprint(\"Training Dataset\\n\", train_df['Class'].value_counts(dropna=False))\nprint(\"Test Dataset\\n\", test_df['Class'].value_counts(dropna=False))\n\nprint(train_df.head(5))\nprint(train_df.tail(5))\nprint(test_df.head(5))\nprint(test_df.tail(5))\nprint(min_val, max_val)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T22:15:33.201951Z","iopub.execute_input":"2024-12-18T22:15:33.202257Z","iopub.status.idle":"2024-12-18T22:15:33.243431Z","shell.execute_reply.started":"2024-12-18T22:15:33.202228Z","shell.execute_reply":"2024-12-18T22:15:33.242180Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df.set_index('Date', inplace=True)\ntest_df.set_index('Date', inplace=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T22:15:33.245085Z","iopub.execute_input":"2024-12-18T22:15:33.245565Z","iopub.status.idle":"2024-12-18T22:15:33.252338Z","shell.execute_reply.started":"2024-12-18T22:15:33.245515Z","shell.execute_reply":"2024-12-18T22:15:33.251152Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from random import random, seed\nseed(42)\ndef generate_image(sample_df, save_location_template, fname, sample_stay=False):\n  class_name = sample_df.iloc[-1,4] # class of the last operation in the sequence\n  roll = 1\n  if sample_stay and class_name == 'stay':\n    roll = random()\n  if roll > 0.8: # only keep 20% of the stay samples because we have so many\n    fplt.plot(\n        sample_df,\n        type='candle',\n        style = \"yahoo\",\n      figratio=(1,1),axisoff=True,tight_layout = True,mav=(2,4,6),\n        savefig=dict(fname=save_location_template.format(class_name, fname),dpi=100),\n        #ylim=(min_val,max_val)\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T22:15:33.253893Z","iopub.execute_input":"2024-12-18T22:15:33.254327Z","iopub.status.idle":"2024-12-18T22:15:33.267749Z","shell.execute_reply.started":"2024-12-18T22:15:33.254273Z","shell.execute_reply":"2024-12-18T22:15:33.266707Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import glob\nfrom random import shuffle\nmatplotlib.use('module://ipykernel.pylab.backend_inline')\nbuy_fnames = glob.glob('/kaggle/input/fortrademl/AI_forex_fisichella_garolla_master/content/Train/buy/*.png')\nsell_fnames = glob.glob('/kaggle/input/fortrademl/AI_forex_fisichella_garolla_master/content/Train/sell/*.png')\nstay_fnames = glob.glob('/kaggle/input/fortrademl/AI_forex_fisichella_garolla_master/content/Train/stay/*.png')\nshuffle(buy_fnames)\nshuffle(sell_fnames)\nshuffle(stay_fnames)\n\nROWS = 4\nCOLS = 4\n\nfig, axes = plt.subplots(ROWS, COLS)\nfig.suptitle('BUY')\nfor r in range(ROWS):\n  for c in range(COLS):\n    im = plt.imread(buy_fnames[r*ROWS+c])\n    axes[r][c].set_axis_off()\n    axes[r][c].patch.set_edgecolor('black')  \n    axes[r][c].patch.set_linewidth('1')  \n    axes[r][c].imshow(im)\n    \n\nfig, axes = plt.subplots(ROWS, COLS)\nfig.suptitle('STAY')\nfor r in range(ROWS):\n  for c in range(COLS):\n    im = plt.imread(stay_fnames[r*ROWS+c])\n    axes[r][c].set_axis_off()\n    axes[r][c].patch.set_edgecolor('black')  \n    axes[r][c].patch.set_linewidth('1')  \n    axes[r][c].imshow(im)\n\nfig, axes = plt.subplots(ROWS, COLS)\nfig.suptitle('SELL')\nfor r in range(ROWS):\n  for c in range(COLS):\n    im = plt.imread(sell_fnames[r*ROWS+c])\n    axes[r][c].set_axis_off()\n    axes[r][c].patch.set_edgecolor('black')  \n    axes[r][c].patch.set_linewidth('1')  \n    axes[r][c].imshow(im)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T22:15:33.269228Z","iopub.execute_input":"2024-12-18T22:15:33.269530Z","iopub.status.idle":"2024-12-18T22:15:37.537169Z","shell.execute_reply.started":"2024-12-18T22:15:33.269502Z","shell.execute_reply":"2024-12-18T22:15:37.536142Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def mlp(x, hidden_units, dropout_rate):\n  for units in hidden_units:\n    x = layers.Dense(units, activation=tf.nn.gelu)(x)\n    x = layers.Dropout(dropout_rate)(x)\n  return x\n\nclass Patches(layers.Layer):\n  def __init__(self, patch_size):\n    super(Patches, self).__init__()\n    self.patch_size = patch_size\n\n  def call(self, images):\n    batch_size = tf.shape(images)[0]\n    patches = tf.image.extract_patches(\n      images=images,\n      sizes=[1, self.patch_size, self.patch_size, 1],\n      strides=[1, self.patch_size, self.patch_size, 1],\n      rates=[1, 1, 1, 1],\n      padding=\"VALID\",\n    )\n    patch_dims = patches.shape[-1]\n    patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n    return patches\n  \nclass PatchEncoder(layers.Layer):\n  def __init__(self, num_patches, projection_dim):\n    super(PatchEncoder, self).__init__()\n    self.num_patches = num_patches\n    self.projection = layers.Dense(units=projection_dim)\n    self.position_embedding = layers.Embedding(\n      input_dim=num_patches, output_dim=projection_dim\n    )\n\n  def call(self, patch):\n    positions = tf.range(start=0, limit=self.num_patches, delta=1)\n    encoded = self.projection(patch) + self.position_embedding(positions)\n    return encoded\n\ndef create_vit_classifier(input_shape, data_preprocessing):\n  inputs = layers.Input(shape=input_shape)\n  # Augment data.\n  augmented = data_preprocessing(inputs)\n  # Create patches.\n  patches = Patches(patch_size)(augmented)\n  # Encode patches.\n  encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n\n  # Create multiple layers of the Transformer block.\n  for _ in range(transformer_layers):\n    # Layer normalization 1.\n    x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n    # Create a multi-head attention layer.\n    attention_output = layers.MultiHeadAttention(\n        num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n    )(x1, x1)\n    # Skip connection 1.\n    x2 = layers.Add()([attention_output, encoded_patches])\n    # Layer normalization 2.\n    x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n    # MLP.\n    x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n    # Skip connection 2.\n    encoded_patches = layers.Add()([x3, x2])\n\n  # Create a [batch_size, projection_dim] tensor.\n  representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n  representation = layers.Flatten()(representation)\n  representation = layers.Dropout(0.5)(representation)\n  # Add MLP.\n  features = mlp(representation, hidden_units=mlp_head_units, dropout_rate=0.5)\n  # Classify outputs.\n  logits = layers.Dense(3)(features)\n  #logits = layers.Dense(1, activation='tanh')(features)\n  # Create the Keras model.\n  model = keras.Model(inputs=inputs, outputs=logits)\n  return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T22:15:37.538859Z","iopub.execute_input":"2024-12-18T22:15:37.539151Z","iopub.status.idle":"2024-12-18T22:15:37.552424Z","shell.execute_reply.started":"2024-12-18T22:15:37.539123Z","shell.execute_reply":"2024-12-18T22:15:37.551433Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## Test patch generation\nmatplotlib.use('module://ipykernel.pylab.backend_inline')\nimport matplotlib.pyplot as plt\n\nIMAGE_SIZE=224\nPATCH_SIZE=16\n\nplt.figure(figsize=(4, 4))\nimage = np.asarray(plt.imread('/kaggle/input/fortrademl/AI_forex_fisichella_garolla_master/content/Train/buy/yahoo_10007.png'))[...,:3]\nplt.imshow(image)\nplt.axis(\"off\")\n\nresized_image = tf.image.resize(\n    tf.convert_to_tensor([image]), size=(IMAGE_SIZE, IMAGE_SIZE)\n)\npatches = Patches(PATCH_SIZE)(resized_image)\nprint(f\"Image size: {IMAGE_SIZE} X {IMAGE_SIZE}\")\nprint(f\"Patch size: {PATCH_SIZE} X {PATCH_SIZE}\")\nprint(f\"Patches per image: {patches.shape[1]}\")\nprint(f\"Elements per patch: {patches.shape[-1]}\")\n\nn = int(np.sqrt(patches.shape[1]))\nplt.figure(figsize=(4, 4), facecolor=(0,0,0))\nfor i, patch in enumerate(patches[0]):\n    ax = plt.subplot(n, n, i + 1)\n    patch_img = tf.reshape(patch, (PATCH_SIZE, PATCH_SIZE, 3))\n    plt.imshow(patch_img.numpy())\n    plt.axis(\"off\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T22:15:37.553860Z","iopub.execute_input":"2024-12-18T22:15:37.554360Z","iopub.status.idle":"2024-12-18T22:15:44.867689Z","shell.execute_reply.started":"2024-12-18T22:15:37.554317Z","shell.execute_reply":"2024-12-18T22:15:44.866614Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"learning_rate = 0.001\nweight_decay = 0.0001\nbatch_size = 64\nnum_epochs = 65\nimage_size = 224  # We'll resize input images to this size\npatch_size = 16   # Size of the patches to be extracted from the input images\nnum_patches = (image_size // patch_size) ** 2\nprojection_dim = 64\nnum_heads = 4\ntransformer_units = [\n    projection_dim * 2,\n    projection_dim,\n]  # Size of the transformer layers\ntransformer_layers = 8\nmlp_head_units = [2048, 1024]  # Size of the dense layers of the final classifier","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T23:38:15.487743Z","iopub.execute_input":"2024-12-18T23:38:15.488543Z","iopub.status.idle":"2024-12-18T23:38:15.494259Z","shell.execute_reply.started":"2024-12-18T23:38:15.488505Z","shell.execute_reply":"2024-12-18T23:38:15.493167Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import glob\nfrom PIL import Image\n\nLBL_MAP = {'buy':0,'sell':1,'stay':2}\n\ntrain_fnames = glob.glob('/kaggle/input/fortrademl/AI_forex_fisichella_garolla_master/content/Train/*/*.png')\nval_fnames = glob.glob('/kaggle/input/fortrademl/AI_forex_fisichella_garolla_master/content/Val/*/*.png')\ntest_fnames = glob.glob('/kaggle/input/fortrademl/AI_forex_fisichella_garolla_master/content/Test/*/*.png')\n\nx_train = np.asarray([np.asarray(Image.open(fn).convert('RGB').resize((image_size, image_size))) for fn in tqdm(train_fnames)])\ny_train = keras.utils.to_categorical(np.asarray([LBL_MAP[fn.split('/')[-2]] for fn in train_fnames]), num_classes=3)\n\nx_val   = np.asarray([np.asarray(Image.open(fn).convert('RGB').resize((image_size, image_size))) for fn in tqdm(val_fnames)])\ny_val   = keras.utils.to_categorical(np.asarray([LBL_MAP[fn.split('/')[-2]] for fn in val_fnames]), num_classes=3)\n\nx_test   = np.asarray([np.asarray(Image.open(fn).convert('RGB').resize((image_size, image_size))) for fn in tqdm(test_fnames)])\ny_test   = keras.utils.to_categorical(np.asarray([LBL_MAP[fn.split('/')[-2]] for fn in test_fnames]), num_classes=3)\n\n\ndata_preprocessing = keras.Sequential(\n    [\n        layers.experimental.preprocessing.Normalization()\n    ],\n    name=\"data_preprocessing\",\n)\n# Compute the mean and the variance of the training data for normalization.\ndata_preprocessing.layers[0].adapt(x_train)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T22:37:53.011370Z","iopub.execute_input":"2024-12-18T22:37:53.012210Z","iopub.status.idle":"2024-12-18T22:39:22.091271Z","shell.execute_reply.started":"2024-12-18T22:37:53.012172Z","shell.execute_reply":"2024-12-18T22:39:22.090159Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\n\ndef run_experiment(model, x_train, y_train, x_val, y_val, x_test, y_test, model_name):\n    optimizer = tfa.optimizers.AdamW(\n        learning_rate=learning_rate, weight_decay=weight_decay\n    )\n\n    model.compile(\n        optimizer=optimizer,\n        loss=keras.losses.CategoricalCrossentropy(from_logits=True),\n        metrics=[\n            keras.metrics.CategoricalAccuracy(name=\"accuracy\"),\n        ],\n    )\n\n    checkpoint_filepath = f\"/kaggle/working/{model_name}/checkpoint\"\n    checkpoint_callback = keras.callbacks.ModelCheckpoint(\n        checkpoint_filepath,\n        monitor=\"val_accuracy\",\n        save_best_only=True,\n        save_weights_only=True,\n    )\n\n    class_weights = {\n    0: 3.0,  # 'Stay' (more weight due to low samples)\n    1: 3.0,  # 'Buy'\n    2: 2.0   # 'Sell' (reduce weight)\n    }\n    history = model.fit(\n        x=x_train,\n        y=y_train,\n        batch_size=batch_size,\n        epochs=num_epochs,\n        validation_data=(x_val,y_val),\n        validation_batch_size=batch_size,\n        callbacks=[checkpoint_callback],\n        shuffle=True,\n        class_weight=class_weights\n    )\n\n    model.load_weights(checkpoint_filepath)\n    _, accuracy = model.evaluate(x_train, y_train)\n    print('\\nPerformance of best model:')\n    print(f\"> Train accuracy: {accuracy}\")\n    _, accuracy = model.evaluate(x_val, y_val)\n    print(f\"> Val accuracy: {accuracy}\")\n    _, accuracy = model.evaluate(x_test, y_test)\n    print(f\"> Test accuracy: {accuracy}\")\n\n    y_pred = model.predict(x_test)\n    cm = confusion_matrix(y_test.argmax(axis=1), y_pred.argmax(axis=1))\n    print('Confusion Matrix:')\n    print(cm)\n\n    return history\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T22:39:23.987776Z","iopub.execute_input":"2024-12-18T22:39:23.988175Z","iopub.status.idle":"2024-12-18T22:39:23.997742Z","shell.execute_reply.started":"2024-12-18T22:39:23.988143Z","shell.execute_reply":"2024-12-18T22:39:23.996609Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ViT","metadata":{}},{"cell_type":"code","source":"vit_classifier = create_vit_classifier((image_size, image_size, 3),data_preprocessing)\nhistory = run_experiment(vit_classifier, x_train, y_train, x_val, y_val, x_test, y_test, 'ViT_224')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T22:17:36.621761Z","iopub.execute_input":"2024-12-18T22:17:36.622227Z","iopub.status.idle":"2024-12-18T22:29:21.029973Z","shell.execute_reply.started":"2024-12-18T22:17:36.622182Z","shell.execute_reply":"2024-12-18T22:29:21.028764Z"},"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Saving the model\nvit_classifier.save(\"vit_model\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T22:29:21.031317Z","iopub.execute_input":"2024-12-18T22:29:21.031681Z","iopub.status.idle":"2024-12-18T22:29:35.553372Z","shell.execute_reply.started":"2024-12-18T22:29:21.031647Z","shell.execute_reply":"2024-12-18T22:29:35.552501Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n\n# 1. Predict probabilities and take argmax to get the class predictions\ny_pred = vit_classifier.predict(x_test)  # Predict\ny_pred_classes = y_pred.argmax(axis=1)   # Convert probabilities to class labels\ny_true_classes = y_test.argmax(axis=1)   # True class labels\n\n# 2. Define class labels\nclass_labels = ['Stay', 'Buy', 'Sell']  # Custom labels for your classes\n\n# 3. Generate confusion matrix\ncm = confusion_matrix(y_true_classes, y_pred_classes)\n\n# 4. Display the confusion matrix with labels\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_labels)\ndisp.plot(cmap=plt.cm.Blues)\nplt.title(\"Confusion Matrix - Buy, Sell, Stay\")\nplt.show()\n\n# 5. Print detailed classification report\nprint(\"Classification Report:\")\nprint(classification_report(y_true_classes, y_pred_classes, target_names=class_labels))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T22:29:35.556848Z","iopub.execute_input":"2024-12-18T22:29:35.557240Z","iopub.status.idle":"2024-12-18T22:29:39.731289Z","shell.execute_reply.started":"2024-12-18T22:29:35.557207Z","shell.execute_reply":"2024-12-18T22:29:39.730180Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ResNet50","metadata":{}},{"cell_type":"code","source":"x_train_resnet = tf.keras.applications.resnet.preprocess_input(x_train)\nx_val_resnet = tf.keras.applications.resnet.preprocess_input(x_val)\nx_test_resnet = tf.keras.applications.resnet.preprocess_input(x_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T22:52:59.023263Z","iopub.execute_input":"2024-12-18T22:52:59.023668Z","iopub.status.idle":"2024-12-18T22:53:04.655654Z","shell.execute_reply.started":"2024-12-18T22:52:59.023625Z","shell.execute_reply":"2024-12-18T22:53:04.654822Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Delete the non-resnet x data to save memory\n# del x_train\n# del x_val\n# del x_test","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T22:30:12.014892Z","iopub.execute_input":"2024-12-18T22:30:12.015272Z","iopub.status.idle":"2024-12-18T22:30:12.025068Z","shell.execute_reply.started":"2024-12-18T22:30:12.015241Z","shell.execute_reply":"2024-12-18T22:30:12.023877Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"resnet_classifier = tf.keras.models.Sequential([\n  tf.keras.applications.ResNet50(include_top=False, input_shape=(224,224,3)),\n  layers.GlobalAveragePooling2D(name='avg_pool'),\n  layers.Dense(3, name='logits')\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T22:53:18.013189Z","iopub.execute_input":"2024-12-18T22:53:18.013563Z","iopub.status.idle":"2024-12-18T22:53:20.373461Z","shell.execute_reply.started":"2024-12-18T22:53:18.013532Z","shell.execute_reply":"2024-12-18T22:53:20.372623Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"history = run_experiment(resnet_classifier, x_train_resnet, y_train, x_val_resnet, y_val, x_test_resnet, y_test, 'ResNet50')\nresnet_classifier.save(\"ResNet50\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T22:53:23.627026Z","iopub.execute_input":"2024-12-18T22:53:23.627413Z"},"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Performing Undersampling","metadata":{}},{"cell_type":"code","source":"# Function to perform undersampling\ndef undersample_data(x, y, target_class, target_count):\n    indices = np.where(y.argmax(axis=1) == target_class)[0]\n    np.random.shuffle(indices)\n    selected_indices = indices[:target_count]\n    return x[selected_indices], y[selected_indices]\n\n# Determine the target count for all classes (equal to the minority class count)\ntarget_count = min(np.sum(y_train.argmax(axis=1) == i) for i in range(3))\n\n# Undersample the SELL class (class 2)\nx_train_sell, y_train_sell = undersample_data(x_train, y_train, target_class=2, target_count=target_count)\n\n# Keep all samples of the BUY and STAY classes\nx_train_buy = x_train[y_train.argmax(axis=1) == 1]\ny_train_buy = y_train[y_train.argmax(axis=1) == 1]\nx_train_stay = x_train[y_train.argmax(axis=1) == 0]\ny_train_stay = y_train[y_train.argmax(axis=1) == 0]\n\n# Combine the undersampled and original data to form the balanced dataset\nx_train_balanced = np.concatenate([x_train_buy, x_train_sell, x_train_stay], axis=0)\ny_train_balanced = np.concatenate([y_train_buy, y_train_sell, y_train_stay], axis=0)\n\n# Shuffle the balanced training dataset\nindices = np.arange(x_train_balanced.shape[0])\nnp.random.shuffle(indices)\nx_train_balanced = x_train_balanced[indices]\ny_train_balanced = y_train_balanced[indices]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T22:39:57.998242Z","iopub.execute_input":"2024-12-18T22:39:57.998900Z","iopub.status.idle":"2024-12-18T22:40:00.375552Z","shell.execute_reply.started":"2024-12-18T22:39:57.998861Z","shell.execute_reply":"2024-12-18T22:40:00.374450Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Analyze the balanced class distribution\nprint(\"Balanced Class Distribution:\")\nprint(pd.DataFrame({'Class': [0, 1, 2], 'Count': np.sum(y_train_balanced, axis=0)}))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T22:40:03.408115Z","iopub.execute_input":"2024-12-18T22:40:03.409286Z","iopub.status.idle":"2024-12-18T22:40:03.416760Z","shell.execute_reply.started":"2024-12-18T22:40:03.409246Z","shell.execute_reply":"2024-12-18T22:40:03.415567Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ViT","metadata":{}},{"cell_type":"code","source":"vit_classifier = create_vit_classifier((image_size, image_size, 3), data_preprocessing)\nhistory_vit = run_experiment(\n    vit_classifier,\n    x_train_balanced,\n    y_train_balanced,\n    x_val,\n    y_val,\n    x_test,\n    y_test,\n    \"ViT_224_Balanced\",\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T22:40:12.607300Z","iopub.execute_input":"2024-12-18T22:40:12.608068Z","iopub.status.idle":"2024-12-18T22:51:51.928988Z","shell.execute_reply.started":"2024-12-18T22:40:12.608031Z","shell.execute_reply":"2024-12-18T22:51:51.927859Z"},"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"vit_classifier.save(\"vit_model_balanced\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T22:52:30.222734Z","iopub.execute_input":"2024-12-18T22:52:30.223113Z","iopub.status.idle":"2024-12-18T22:52:43.864888Z","shell.execute_reply.started":"2024-12-18T22:52:30.223082Z","shell.execute_reply":"2024-12-18T22:52:43.863989Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Resnet50","metadata":{}},{"cell_type":"code","source":"# Prepare ResNet50 inputs\nx_train_resnet_balanced = tf.keras.applications.resnet.preprocess_input(x_train_balanced)\nx_val_resnet = tf.keras.applications.resnet.preprocess_input(x_val)\nx_test_resnet = tf.keras.applications.resnet.preprocess_input(x_test)\n\n# Train the ResNet50 model\nresnet_classifier = tf.keras.models.Sequential([\n    tf.keras.applications.ResNet50(include_top=False, input_shape=(224, 224, 3)),\n    layers.GlobalAveragePooling2D(name=\"avg_pool\"),\n    layers.Dense(3, name=\"logits\"),\n])\nhistory_resnet = run_experiment(\n    resnet_classifier,\n    x_train_resnet_balanced,\n    y_train_balanced,\n    x_val_resnet,\n    y_val,\n    x_test_resnet,\n    y_test,\n    \"ResNet50_Balanced\",\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T23:38:29.565774Z","iopub.execute_input":"2024-12-18T23:38:29.566179Z"},"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"resnet_classifier.save(\"ResNet50_balanced\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Piplining","metadata":{}},{"cell_type":"code","source":"# import pandas as pd\n# import numpy as np\n# import tensorflow as tf\n# from tensorflow import keras\n# import glob\n# from PIL import Image\n# import mplfinance as fplt\n# import matplotlib.pyplot as plt\n# from sklearn.preprocessing import LabelEncoder","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T22:29:39.774089Z","iopub.execute_input":"2024-12-18T22:29:39.774494Z","iopub.status.idle":"2024-12-18T22:29:39.783909Z","shell.execute_reply.started":"2024-12-18T22:29:39.774437Z","shell.execute_reply":"2024-12-18T22:29:39.782924Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# model = tf.keras.models.load_model('/kaggle/working/vit_model')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T22:29:39.785250Z","iopub.execute_input":"2024-12-18T22:29:39.785710Z","iopub.status.idle":"2024-12-18T22:29:39.797100Z","shell.execute_reply.started":"2024-12-18T22:29:39.785657Z","shell.execute_reply":"2024-12-18T22:29:39.796170Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Load data","metadata":{}},{"cell_type":"code","source":"# new_data = pd.read_csv('/kaggle/input/new-data-m1/data')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T22:29:39.798403Z","iopub.execute_input":"2024-12-18T22:29:39.798857Z","iopub.status.idle":"2024-12-18T22:29:39.810370Z","shell.execute_reply.started":"2024-12-18T22:29:39.798776Z","shell.execute_reply":"2024-12-18T22:29:39.809437Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import os\n# import pandas as pd\n# import mplfinance as fplt\n\n# import pandas as pd\n\n# def preprocess_new_data(df):\n#     # Check and rename columns to match required format\n#     required_columns = ['time', 'bid_o', 'bid_h', 'bid_l', 'bid_c', 'ask_o', 'ask_h', 'ask_l', 'ask_c']\n#     if not all(col in df.columns for col in required_columns):\n#         raise KeyError(f\"Missing required columns: {set(required_columns) - set(df.columns)}\")\n    \n#     # Create 'Date' column from 'time' (ensure datetime format)\n#     df['Date'] = pd.to_datetime(df['time'])\n    \n#     # Create OHLC columns (average of bid and ask prices)\n#     df['Open'] = (df['bid_o'] + df['ask_o']) / 2\n#     df['High'] = (df['bid_h'] + df['ask_h']) / 2\n#     df['Low'] = (df['bid_l'] + df['ask_l']) / 2\n#     df['Close'] = (df['bid_c'] + df['ask_c']) / 2\n\n#     # Select only the required columns for candlestick plotting\n#     df = df[['Date', 'Open', 'High', 'Low', 'Close']]\n\n#     # Set 'Date' as the index\n#     df.set_index('Date', inplace=True)\n\n#     return df\n\n\n\n# def generate_candlestick_images(df, save_image_path):\n#     # Ensure the save path exists\n#     if not os.path.exists(save_image_path):\n#         os.makedirs(save_image_path)\n    \n#     for idx in range(len(df)):\n#         # Select the last 50 rows (candles)\n#         candle_data = df.iloc[max(0, idx-49):idx+1]\n        \n#         # Generate and save the candlestick chart\n#         save_file = os.path.join(save_image_path, f\"candle_{idx}.png\")\n#         fplt.plot(\n#             candle_data,\n#             type='candle',\n#             style='yahoo',\n#             savefig=dict(fname=save_file, dpi=100)\n#         )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T22:29:39.811919Z","iopub.execute_input":"2024-12-18T22:29:39.812730Z","iopub.status.idle":"2024-12-18T22:29:39.823923Z","shell.execute_reply.started":"2024-12-18T22:29:39.812675Z","shell.execute_reply":"2024-12-18T22:29:39.822956Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import cv2\n# import numpy as np\n\n# def predict_actions(image_folder):\n#     predictions = []\n#     image_files = sorted(os.listdir(image_folder))  # Sort by file name for consistency\n    \n#     for image_file in image_files:\n#         image_path = os.path.join(image_folder, image_file)\n        \n#         # Load the image and preprocess it for the model\n#         image = cv2.imread(image_path)\n#         image = cv2.resize(image, (224, 224))  # Assuming model expects 224x224 input\n#         image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert to RGB\n#         image = image / 255.0  # Normalize pixel values\n#         image = np.expand_dims(image, axis=0)  # Add batch dimension\n        \n#         # Predict the action\n#         prediction = model.predict(image)\n#         action = np.argmax(prediction)  # Get the index of the highest probability\n        \n#         # Map the index to the action\n#         action_map = {0: \"stay\", 1: \"buy\", 2: \"sell\"}\n#         predictions.append(action_map[action])\n    \n#     return predictions","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T22:29:39.825184Z","iopub.execute_input":"2024-12-18T22:29:39.825612Z","iopub.status.idle":"2024-12-18T22:29:39.835931Z","shell.execute_reply.started":"2024-12-18T22:29:39.825554Z","shell.execute_reply":"2024-12-18T22:29:39.835002Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Preprocess the data\n# preprocessed_data = preprocess_new_data(new_data)\n\n# # Generate candlestick images\n# generate_candlestick_images(preprocessed_data, save_image_path=\"/kaggle/working/images\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T22:29:39.837297Z","iopub.execute_input":"2024-12-18T22:29:39.837685Z","iopub.status.idle":"2024-12-18T22:29:39.850998Z","shell.execute_reply.started":"2024-12-18T22:29:39.837642Z","shell.execute_reply":"2024-12-18T22:29:39.850062Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# predictions = predict_actions(\"/kaggle/working/images\")\n# print(\"Predicted Actions:\", predictions)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T22:29:39.852296Z","iopub.execute_input":"2024-12-18T22:29:39.852651Z","iopub.status.idle":"2024-12-18T22:29:39.862093Z","shell.execute_reply.started":"2024-12-18T22:29:39.852599Z","shell.execute_reply":"2024-12-18T22:29:39.860848Z"}},"outputs":[],"execution_count":null}]}